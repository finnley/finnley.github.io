+++
title = '倒排索引与分词'
date = 2020-05-23T14:29:51+08:00
draft = true
categories = [ "ElasticSearch" ]
tags = [ "es" ]
+++

# 目录与索引

## 目录

对于书本，一般都会有目录，它的作用就是可以快速让用户知道它在讲什么，并方便用户定位想看的内容，用户可以方便查看章节，还有指明章节的名称和页码

**如何快速查找一个关键词所在的页面？**

有些书还有可能会有索引，它会将一些关键词列举出来，列出它们在哪一页出现，方便用户查找

书与搜索引擎类似，目录页对应的正排索引，而索引页对应倒排索引

## 正排索引

可以通过文档ID获取到文档的内容，或者单词的关联关系，这里的单词就是将文档分词之后每个分词的结果

正排索引维护的是文档id到文档内容、单词的关联关系

|   文档ID  |   文档内容    |
|   :----:  |   :----:   |
|   1   |   elasticsearch 是最流行的搜索引擎    |
|   2   |   php 是世界上最好的语言  |
|   3   |   搜索引擎是如何诞生的    |       

正排索引文档ID都对应了一个文档内容，即可以通过文档ID获取文档的内容

## 倒排索引

维护的是单词到文档id的关联关系

|   单词  |   文档ID列表    |
|   :----:  |   :----:   |
|   elasticsearch   |   1   |
|   流行    |   1   |
|   搜索引擎    |   1,3 |
|   php |   2   |
|   世界    |   2   |
|   最好  |   2   |
|   语言    |   2   |
|   如何    |   3   |
|   诞生    |   3   |

**倒排索引如何产生的呢？**

是将正排索引做了分词，比如：
将 `elasticsearch是最流行的搜索引擎` 分成了 `elasticsearch`, `流行`, `搜索引擎` 3个单词，这3个单词就和文档ID 1 关联起来了；
`php 是世界上最好的语言` 分成了个 `php`, `世界`, `最好`, `语言`，它们和文档ID 2 关联起来了；
`搜索引擎是如何诞生的` 分成了 `搜索引擎`, `如何`, `诞生`，它们和文档ID 3 关联起来了；
可以看到搜索引擎关联了两个文档

##  查询流程

**有了正排和倒排之后如何去实现一个搜索引擎呢？**

比如查询包含 `搜索引擎` 的文档

1. 先通过倒排索引获得 `索引引擎` 对应的文档ID，有1和3
2. 返回给用的肯定是文档的原始内容，所以再通过正排索引查询 1和3 的完整内容
3. 返回用户最终结果

上面就是正排索引和倒排索引结合在一起实现搜索查询的流程

# 倒排索引组成

倒排索引是搜索引擎的核心，主要包含两部分：

- 单词词典 (Term Dictionary)
- 倒排列表 (Posting List)

## 单词词典 (Term Dictionary)

1. 记录所有文档的单词，一般比较大，当存储内容非常大时，单词词典也会很大，因为分词结果很多
2. 记录单词到倒排列表的关联信息（其实就是一个单词关联了哪些文档，记录了关联信息，这样才能找到关联的文档的ID，然后通过ID拿到对应信息 ）

单词字典的实现一般使用 `B + Tree` ，下图排序采用拼音实现，构造方法参考：
https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html

![](https://images.notes.xuepincat.com/elastic/elasticsearch/13.png)

B + Tree 的好处就是插入和查询性能非常高，可以充分利用磁盘和内存的映射机制，单词字典使用了这种数据结构可以能保证它的高效

## 倒排列表 (Posting List)

1. 记录的是单词对应文档的集合，由倒排索引项（Posting）组成
2. 倒排索引项（Posting）主要包含下面内容：
    * 文档ID，主要获取原始信息
    * 单词频率（TF，Term Frequency），记录该单词在给文档中出现的次数，用于后续相关性算分
    * 位置（Position），记录单词在文档中的分词位置（多个），用于做词语搜索（Phrase Query），比如搜索苹果手机，苹果手机可能被分词成 `苹果` 和 `手机` 两个词，拿这两个词去搜索的时候，只给苹果在手机前面的文档，不需要手机在苹果前面的
    * 迁移（Offset），记录单词在文档的开始和结束位置，用于做高亮显示

以 `搜索引擎` 为例，看如何构建 Posting List

|   文档ID  |   文档内容    |
|   :----:  |   :----:   |
|   1   |   elasticsearch是最流行的搜索引擎   |
|   2    |   php 是世界上最好的语言   |
|   3    |   搜索引擎是如何诞生的 |

转换为下面的 Posting List

|   DocID  |   TF    |  Position    |   Offset  |
|   :----:  |   :----:   |   :----:   |   :----:   |
|   1   |   1   |   2   |   <18,22>   |
|   3    |   1   |  0   |   <0,4>   |

从上面的表中看到搜索引擎有连个文档1和3，下面的 Posting List,每一行都是一个倒排索引项
第一行DocID表示文档ID是1的文档，频率是1，因为`搜索引擎`在DocID是1的文档中出现了1次，

Position表示分词后出现的位置，`elasticsearch是最流行的搜索引擎` 被分词成了 `elasticsearch`, `流行`, `搜索引擎` 3个单词，搜索引擎在第二个位置，从0开始计算的

Offset表示搜索引擎在这个文档中的字符的位置，是从18到22

第二行DocID表示文档ID是3的文档，频率是1，因为`搜索引擎`在DocID是1的文档中出现了1次

Position表示分词后出现的位置，`搜索引擎是如何诞生的` 被分词成了 `elasticsearch`, `搜索引擎`, `如何`， `诞生` 3个单词，搜索引擎在第一个位置，从0开始计算的

Offset起始位置是0到4

一个倒排索引大致的样子，单词字典与倒排列表整合一起结构如下：

![](https://images.notes.xuepincat.com/elastic/elasticsearch/14.png)

这里的倒排索引项值显示了 DocID

倒排索引是由 Term Dictionary 和 Posting List 构成的，Term Dictionary 是一个B+ Tree, Term Dictionary 会记录每个单词在 Posting List中的位置，比如搜索的时候可以通过Term Dictionary定位到搜索引擎，通过搜索引擎可以拿到在 Posting List 中的偏移量，可以快速定位到在 Posting List 中的位置，就拿到了真正的倒排索引项1,3,图中只列出了DocID,但是实际上收到拍索引项是可以包含四项信息的一项，拿到一项就可以去正排索引中拿到原始的信息，计算相关性得分，然后进行排序，返回给用户

es存储的是一个 json 格式的文档，其中包含多个字段，每个字段会有自己的倒排索引，比如

```
{
    "username": "alfred",
    "job": "programmer"
}
```
在构建倒排索引的时候是按照字段进行构建的，也就是username会有一个倒排索引，job也会有一个倒排索引

# 分词

分词是指将文本转换成一系列单词(term or token) 的过程，也可以叫做文本分析，在es里面成为 Analysis

比如有一个文本 `elasticsearch是最流行的搜索引擎`，被分词后的结果是 `elasticsearch`, `流行`, `搜索引擎`

## 分词器

分词器是es中专门处理分词的组件，英文为 Analyzer, 它的组成如下：

**Character Filters**

针对原始文本进行处理，比如去除 html 特殊标记

**Tokenizer**
    
将原始文本按照一定规则切分为单词

**Token Filters**

针对 tokenizer 处理的单词进行再加工，比如转小写，删除或者新增处理

## 调用顺序

![](https://images.notes.xuepincat.com/elastic/elasticsearch/15.png)

Analyzer 有一个先后顺序，从上到下会先经过 Character Filters, Tokenizer, 和 Token Filters

# Analyze API

es 提供了一个测试分词的API接口，方便央行增分词效果，endpoint是_analyze

* 可以直接指定 analyzer 进行测试
* 可以直接指定索引中的字段进行测试 
* 可以自定义分词器进行测试

## 直接指定 analyzer 进行测试

analyzer: 分词器 standard 是es自带的分词器
text: 要测试的分词文本

```
POST _analyze
{
  "analyzer": "standard",
  "text": "hello world!"
}
```

```
{
  "tokens": [
    {
      "token": "hello", # 分词结果
      "start_offset": 0, # 起始偏移
      "end_offset": 5, # 结束偏移
      "type": "<ALPHANUM>",
      "position": 0 # 分词位置
    },
    {
      "token": "world",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/16.png)

## 直接指定索引中的字段进行测试

使用场景：当已经创建了一个索引，针对索引字段做查询的时候输出和预期不一样，这个时候就可以针对这个字段做分词测试

test_index: 测试的索引
field: 测试字段
text: 测试文本

```
# analyze
POST test_index/_analyze
{
  "field": "username",
  "text": "hello world!"
}
```

```
{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "world",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/17.png)

## 自定义分词器进行测试

**不使用filter**

```
# analyze
POST _analyze
{
  "tokenizer": "standard",
  "text": "Hello World!"
}
```

```
{
  "tokens": [
    {
      "token": "Hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "World",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/18.png)

**使用 filter**

```
# analyze
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase"
  ],
  "text": "Hello World!"
}
```

```
{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "world",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/19.png)

# 预定义分词器

es 自带下面这些分词器：

* Standard
* Simple
* Whitespace
* Stop
* Keyword
* Pattern
* Language

## Standard Analyzer

默认分词器

    按词切分，支持多语言，小写处理


![](https://images.notes.xuepincat.com/elastic/elasticsearch/20.png)

- request

```
# analyze
POST _analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "the",
      "start_offset": 0,
      "end_offset": 3,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "2",
      "start_offset": 4,
      "end_offset": 5,
      "type": "<NUM>",
      "position": 1
    },
    {
      "token": "quick",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "brown",
      "start_offset": 12,
      "end_offset": 17,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "foxes",
      "start_offset": 18,
      "end_offset": 23,
      "type": "<ALPHANUM>",
      "position": 4
    },
    {
      "token": "jumped",
      "start_offset": 24,
      "end_offset": 30,
      "type": "<ALPHANUM>",
      "position": 5
    },
    {
      "token": "ober",
      "start_offset": 31,
      "end_offset": 35,
      "type": "<ALPHANUM>",
      "position": 6
    },
    {
      "token": "the",
      "start_offset": 36,
      "end_offset": 39,
      "type": "<ALPHANUM>",
      "position": 7
    },
    {
      "token": "lazy",
      "start_offset": 40,
      "end_offset": 44,
      "type": "<ALPHANUM>",
      "position": 8
    },
    {
      "token": "dog's",
      "start_offset": 45,
      "end_offset": 50,
      "type": "<ALPHANUM>",
      "position": 9
    },
    {
      "token": "bone",
      "start_offset": 51,
      "end_offset": 55,
      "type": "<ALPHANUM>",
      "position": 10
    }
  ]
}
```

## Simple Analyzer

    在那好非字母切分,小写处理

![](https://images.notes.xuepincat.com/elastic/elasticsearch/21.png)

- request

```
# analyze
POST _analyze
{
  "analyzer": "simple",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "the",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "quick",
      "start_offset": 6,
      "end_offset": 11,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 12,
      "end_offset": 17,
      "type": "word",
      "position": 2
    },
    {
      "token": "foxes",
      "start_offset": 18,
      "end_offset": 23,
      "type": "word",
      "position": 3
    },
    {
      "token": "jumped",
      "start_offset": 24,
      "end_offset": 30,
      "type": "word",
      "position": 4
    },
    {
      "token": "ober",
      "start_offset": 31,
      "end_offset": 35,
      "type": "word",
      "position": 5
    },
    {
      "token": "the",
      "start_offset": 36,
      "end_offset": 39,
      "type": "word",
      "position": 6
    },
    {
      "token": "lazy",
      "start_offset": 40,
      "end_offset": 44,
      "type": "word",
      "position": 7
    },
    {
      "token": "dog",
      "start_offset": 45,
      "end_offset": 48,
      "type": "word",
      "position": 8
    },
    {
      "token": "s",
      "start_offset": 49,
      "end_offset": 50,
      "type": "word",
      "position": 9
    },
    {
      "token": "bone",
      "start_offset": 51,
      "end_offset": 55,
      "type": "word",
      "position": 10
    }
  ]
}
```

## Whitespace Analyzer

按照空格切分

![](https://images.notes.xuepincat.com/elastic/elasticsearch/22.png)

- request 

```
# analyze
POST _analyze
{
  "analyzer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "2",
      "start_offset": 4,
      "end_offset": 5,
      "type": "word",
      "position": 1
    },
    {
      "token": "QUICK",
      "start_offset": 6,
      "end_offset": 11,
      "type": "word",
      "position": 2
    },
    {
      "token": "Brown-Foxes",
      "start_offset": 12,
      "end_offset": 23,
      "type": "word",
      "position": 3
    },
    {
      "token": "jumped",
      "start_offset": 24,
      "end_offset": 30,
      "type": "word",
      "position": 4
    },
    {
      "token": "ober",
      "start_offset": 31,
      "end_offset": 35,
      "type": "word",
      "position": 5
    },
    {
      "token": "the",
      "start_offset": 36,
      "end_offset": 39,
      "type": "word",
      "position": 6
    },
    {
      "token": "lazy",
      "start_offset": 40,
      "end_offset": 44,
      "type": "word",
      "position": 7
    },
    {
      "token": "dog's",
      "start_offset": 45,
      "end_offset": 50,
      "type": "word",
      "position": 8
    },
    {
      "token": "bone!",
      "start_offset": 51,
      "end_offset": 56,
      "type": "word",
      "position": 9
    }
  ]
}
```

## Stop Analyzer

Stop Word 是指预期助词等修饰性的词语，比如 the, an, 的， 这等等

    相比 Simple Analyzer 多了 Stop Word 处理

![](https://images.notes.xuepincat.com/elastic/elasticsearch/23.png)

- request

```
# analyze
POST _analyze
{
  "analyzer": "stop",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "quick",
      "start_offset": 6,
      "end_offset": 11,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 12,
      "end_offset": 17,
      "type": "word",
      "position": 2
    },
    {
      "token": "foxes",
      "start_offset": 18,
      "end_offset": 23,
      "type": "word",
      "position": 3
    },
    {
      "token": "jumped",
      "start_offset": 24,
      "end_offset": 30,
      "type": "word",
      "position": 4
    },
    {
      "token": "ober",
      "start_offset": 31,
      "end_offset": 35,
      "type": "word",
      "position": 5
    },
    {
      "token": "lazy",
      "start_offset": 40,
      "end_offset": 44,
      "type": "word",
      "position": 7
    },
    {
      "token": "dog",
      "start_offset": 45,
      "end_offset": 48,
      "type": "word",
      "position": 8
    },
    {
      "token": "s",
      "start_offset": 49,
      "end_offset": 50,
      "type": "word",
      "position": 9
    },
    {
      "token": "bone",
      "start_offset": 51,
      "end_offset": 55,
      "type": "word",
      "position": 10
    }
  ]
}
```

## Keyword Analyzer

    不分词，直接将输入作为一个单词输出

![](https://images.notes.xuepincat.com/elastic/elasticsearch/24.png)

- request

```
# analyze
POST _analyze
{
  "analyzer": "keyword",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!",
      "start_offset": 0,
      "end_offset": 56,
      "type": "word",
      "position": 0
    }
  ]
}
```

## Pattern Analyzer

    通过正则表达式自定义分割符

    默认是 `\W+` ,即非字词的符号作为分隔符

![](https://images.notes.xuepincat.com/elastic/elasticsearch/25.png)

- request

```
# analyze
POST _analyze
{
  "analyzer": "pattern",
  "text": "The 2 QUICK Brown-Foxes jumped ober the lazy dog's bone!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "the",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "2",
      "start_offset": 4,
      "end_offset": 5,
      "type": "word",
      "position": 1
    },
    {
      "token": "quick",
      "start_offset": 6,
      "end_offset": 11,
      "type": "word",
      "position": 2
    },
    {
      "token": "brown",
      "start_offset": 12,
      "end_offset": 17,
      "type": "word",
      "position": 3
    },
    {
      "token": "foxes",
      "start_offset": 18,
      "end_offset": 23,
      "type": "word",
      "position": 4
    },
    {
      "token": "jumped",
      "start_offset": 24,
      "end_offset": 30,
      "type": "word",
      "position": 5
    },
    {
      "token": "ober",
      "start_offset": 31,
      "end_offset": 35,
      "type": "word",
      "position": 6
    },
    {
      "token": "the",
      "start_offset": 36,
      "end_offset": 39,
      "type": "word",
      "position": 7
    },
    {
      "token": "lazy",
      "start_offset": 40,
      "end_offset": 44,
      "type": "word",
      "position": 8
    },
    {
      "token": "dog",
      "start_offset": 45,
      "end_offset": 48,
      "type": "word",
      "position": 9
    },
    {
      "token": "s",
      "start_offset": 49,
      "end_offset": 50,
      "type": "word",
      "position": 10
    },
    {
      "token": "bone",
      "start_offset": 51,
      "end_offset": 55,
      "type": "word",
      "position": 11
    }
  ]
}
```

## Language Analyzer

提供了30+常见的语言分词器

arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, ...

# 中文分词

中文分词是指将一个汉字序列切分成一个一个单独的词。在英文中，单词之间是以空格作为自然分解符，汉语中词没有一个形式上的分界符。

上下文不同，分词结果迥异，比如交叉歧义问题

    - 乒乓球拍/卖/完了
    - 乒乓球/拍卖/完了

https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247486148&amp;idx=1&amp;sn=817027a204650763c1bea3e837d695ea&source=41#wechat_redirect

## IK

常用分词系统

- 实现中英文单词的切分，支持 ik_smart, ik_maxword等模式
- 可自定义词库，支持热更新分词词典
- https://github.com/medcl/elasticsearch-analysis-ik

## jieba

常用分词系统

- python 中流行的分词系统，支持分词和词性标注
- 支持繁体分词，自定义词典，并行分词等
- https://github.com/sing1ee/elasticsearch-jieba-plugin

## Hanlp

基于自然语言处理的分析系统

- 由一些列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用
- https://github.com/hankcs/HanLP

## THULAC

- YHU Lexical Analyzer for Chinese,由清华大学自然语言处理与社会人文计算实验室颜值退出的一套中文此法分析工具包，具有中文分词和词性标注功能
- https://github.com/microbun/elasticsearch-thulac-plugin

# 自定义分词器-CharacterFilter

当自带的分词无法满足需求时，可以自定义分词

- 通过自定义 Character Filters, Tokenizer 和 Token Filter 实现

# Character Filter

- 在 Tokenizer 之前对原始文本进行处理，比如增加，删除或替换字符等
- 自带如下：
    HTML Strip 去除html标签和转换html实体
    Mapping 进行字符替换操作
    Pattern Replace 进行正则匹配替换
- 会影响后续 tokenizer 解析的 position 和 offset 信息

* API

tokenizer: keyword类型的tokenizer可以直接看到输出结果
char filter: 指明要使用的 char_filter,如 html_strip
text: 测试的文本

- request

```
# charater filter
POST _analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    "html_strip"
  ],
  "text": "<p>I&apos;m so<b>happy</b>!</p>"
}
```

- response

```
{
  "tokens": [
    {
      "token": """

I'm sohappy!

""",
      "start_offset": 0,
      "end_offset": 31,
      "type": "word",
      "position": 0
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/26.png)

# 自定义分词-Tokenizer

将原始文本按照一定规则切分为单词 （term or token）

自带如下：

standard: 按照单词进行分割
letter: 按照非字符类进行分割
whitespace: 按照空格进行分割
UAX URL Email: 按照standard分割，但不会分割邮箱和url
NGram 和 Edge NGram: 连词分割
Path Hierarchy: 按照文件路径进行分割

* API

tokenizer: 指定要测试的tokenizer

- request

```
# tokenizer
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}
```

- response

```
{
  "tokens": [
    {
      "token": "/one",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 0
    },
    {
      "token": "/one/two",
      "start_offset": 0,
      "end_offset": 8,
      "type": "word",
      "position": 0
    },
    {
      "token": "/one/two/three",
      "start_offset": 0,
      "end_offset": 14,
      "type": "word",
      "position": 0
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/27.png)

# 自定义分词器-TokenFilter

对于tokenizer输出的单词(term) 进行增加，删除，修改等操作
自带如下：
    lowercase 将所有term转为小写
    stop删除stop words
    NGram 和 Edge Ngram 连词分割
    Synonym添加近义词的term

* API

- request

```
# token filter
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "stop",
    "lowercase",
    {
      "type": "ngram",
      "min_gram": 4,
      "max_gram": 4
    }
  ],
  "text": "a Hello, world!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "hell",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "ello",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "worl",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "orld",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/28.png)

修改 min_gram ,并去掉 max_gram

- request

```
# token filter
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "stop",
    "lowercase",
    {
      "type": "ngram",
      "min_gram": 2
    }
  ],
  "text": "a Hello, world!"
}
```

- response

```
{
  "tokens": [
    {
      "token": "he",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "el",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "ll",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "lo",
      "start_offset": 2,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "wo",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "or",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "rl",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "ld",
      "start_offset": 9,
      "end_offset": 14,
      "type": "<ALPHANUM>",
      "position": 2
    }
  ]
}
```

# 自定义分词API

自定义分词API需要在索引的配置中设定

分词配置可以自定义
char_filter
tokenizer
filter
analyzer

- request

```
# custom analyzer setting
PUT test_index
{
  "settings": {
    "analysis": {
      "char_filter": {},
      "tokenizer": {},
      "filter": {},
      "analyzer": {}
    }
  }
}
```

* 实例1

![](https://images.notes.xuepincat.com/elastic/elasticsearch/29.png)

- request

```
# customize
PUT test_index_1
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  }
}
```

- response

```
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "test_index_1"
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/30.png)

测试

- request

```
POST test_index_1/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "Is this <b> a box</b>?"
}
```

- response

```
{
  "tokens": [
    {
      "token": "is",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 3,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "a",
      "start_offset": 12,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "box",
      "start_offset": 14,
      "end_offset": 21,
      "type": "<ALPHANUM>",
      "position": 3
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/31.png)

* 实例2

![](https://images.notes.xuepincat.com/elastic/elasticsearch/32.png)

- request

```
PUT test_index_2
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": [
            "emoticons"
          ],
          "tokenizer": "punctuation",
          "filter": [
            "lowercase",
            "english_stop"
          ]
        }
      },
      "tokenizer": {
        "punctuation": {
          "type": "pattern",
          "pattern": "[.,!?]"
        }
      },
      "char_filter": {
        "emoticons": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "filter": {
        "english_stop": {
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}
```

- response

```
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "test_index_2"
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/33.png)

测试 

- request

```
POST test_index_2/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "I'm a :) person, and you?"
}
```

- response

```
{
  "tokens": [
    {
      "token": "i'm a _happy_ person",
      "start_offset": 0,
      "end_offset": 15,
      "type": "word",
      "position": 0
    },
    {
      "token": " and you",
      "start_offset": 16,
      "end_offset": 24,
      "type": "word",
      "position": 1
    }
  ]
}
```

![](https://images.notes.xuepincat.com/elastic/elasticsearch/34.png)

# 分词使用时机

- 创建或更新文档时(Index Time)，会对相应的文档进行分词处理
- 查询时(Search Time)，会对查询语句进行分词

在索引的时候已经把所有文档都已经分好词了，那么在查询匹配的时候肯定也要拿着一个一个的单词来匹配才能找到对应的结果，所以查询的语句也是要分词的

## 索引时分词

索引时分词是通过配置 Index Mapping 中每个字段的 anlyzer 属性实现的

* 不指定分词时，使用默认 standard

- request

```
PUT test_index
{
  "mappings": {
    "doc": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "whitespace"
        }
      }
    }
  }
}
```

* 查询时分词

查询的时候通过 analyzer 指定分词器

```
POST test_index/_search
{
  "query": {
    "match": {
      "message": {
        "query": "hello",
        "analyzer": "standard"
      }
    }
  }
}
```

通过 index mapping 设置 search_analyzer 实现

```
PUT test_index
{
  "mappings": {
    "doc": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "whitespace",
          "search_analyzer": "standard"
        }
      }
    }
  }
}
```

    注: 一般不需要特别指定查询时分词器，直接使用索引时分词器即可，否则会出现无法匹配的情况

## 分词使用建议

* 明确字段是否需要分词，不需要分词的字段就将 type 设置为 keyword,可以节省空间和提高写性能

* 善用 _analyze API , 查看文档的具体分词结果

* 动手测试


